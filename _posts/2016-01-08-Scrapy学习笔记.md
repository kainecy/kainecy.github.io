---
layout: post
title: Scrapy学习笔记
---

##创建项目

使用命令`scrapy startproject tutorial`创建一个名为`tutorial`的项目结构：  

````
tutorial/    scrapy.cfg              #部署的配置文件    tutorial/               #项目模块文件夹        __init__.py                 items.py            #项目items文件，定义爬取的数据        pipelines.py        #项目pipelines文件        settings.py         #项目设置文件        spiders/            #用来放置爬虫的文件夹，定义爬取规则            __init__.py
````

集成开发环境我选择[PyCharm](https://www.jetbrains.com/pycharm/download/)，用惯了***jetbrains*** 的东西了。  
直接用PyCharm新建一个项目，选择位置在上面命令创建的位置，库选择2.7的。  

##写一个简单爬虫

在`items.py`文件中定义item:

```
import scrapy

class DmozItem(scrapy.Item):
    title = scrapy.Field()
    link = scrapy.Field()
    desc = scrapy.Field()
```

在spiders文件夹中定义一个爬虫`DmozSpider.py`:(***注意第一行的utf-8编码设置***)  

```
#coding=utf-8

import scrapy

class DmozSpider(scrapy.Spider):
    name = "dmoz"           #name属性很重要，不同spider不能使用相同的name
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
    ]
    def parse(self, response):
        filename = response.url.split("/")[-2] + '.html'
        with open(filename, 'wb') as f:
            f.write(response.body)
```

使用命令运行爬虫`scrapy crawl dmoz`。  
运行后模块下会多出两个`html`文件来。  


##命令行工具

###全局命令

> * startproject #scrapy startproject \<project_name\>创建项目
> * settings
> * runspider #scrapy runspider \<spider_file.py\>运行在一个Python文件中定义的独立爬虫
> * shell
> * fetch #scrapy fetch \<url\>下载url内容输出到标准输出
> * view #scrapy view \<url\>在浏览器中打开url
> * version

###项目命令
只能在项目目录执行

> * crawl #scrapy crawl \<spider\>使用一个爬虫开始爬
> * check
> * list #scrapy list列出该项目下所有爬虫
> * edit #scrapy edit \<spider\>调用`setting`中设置的编辑器进行编辑
> * parse
> * genspider #scrapy genspider [-t template] \<name\> \<domain\>在当前项目中生成一个爬虫
> * bench


##爬虫
一个爬虫就是一套规则，它定义了如何进行抓取，如何提取结构化数据等：  
> 1. 生成请求抓取一个网址，并使用回调函数处理抓取回来的返回信息。默认使用的方法`start_requests()`、`parse()`。
> 2. 在回调函数中，解析返回的网页信息，返回提取的数据、`Item`对象、`Request`对象。
> 3. 最后，返回的数据被存储到数据库或者文件系统。  

###scrapy.Spider
`scrapy.spiders.Spider`这是一个最简单的爬虫，其他爬虫必须继承这个。它提供一个默认的`start_request()`方法实现，该方法从`start_urls`属性开始爬取数据，然后调用`parse`方法解析返回的结果。

1. name 必需，定义爬虫的名字
2. allowed_domains 可选，包含了这个爬虫可以爬取的域。`OffsiteMiddleware`开启的时候，不是在这些域下的URL将不会被继续抓取。
3. start_urls 没有特殊的URL被指定时，爬虫将从这些URL开始爬取。
4. custom_settings 执行这个爬虫时加载的设置，可以覆盖项目的设置。
5. crawler 
6. settings 
7. logger 
8. from_crawler 
9. start_requests()
10. make_requests_from_url(url)
11. parse(response)
12. log(message[,level,component])
13. closed(reason)

一个简单的爬虫例子：  

```
import scrapy
class MySpider(scrapy.Spider):    name = 'example.com' allowed_domains = ['example.com'] 
    start_urls = [        'http://www.example.com/1.html',        'http://www.example.com/2.html',        'http://www.example.com/3.html',    ]    def parse(self, response):        for h3 in response.xpath('//h3').extract():            yield {"title": h3}
                    for url in response.xpath('//a/@href').extract(): 
            yield scrapy.Request(url, callback=self.parse)
```

###Spider参数

爬虫可以接收一些参数，可以通过`crawl`命令使用`-a`: 
 
```
scrapy crawl myspider -a category=electronics
```
爬虫在构造器中接收这些参数：

```
import scrapy
class MySpider(scrapy.Spider):    name = 'myspider'    def __init__(self, category=None, *args, **kwargs): 
        super(MySpider, self).__init__(*args, **kwargs)
        self.start_urls = ['http://www.example.com/categories/%s' % category] 
        # ...
```

###通用Spiders

有一些通用的Spiders，可以通过继承他们，通过定义子类的方式来使用他们中的功能。  
定义一个`TestItem`在items模块中：  

```
import scrapy
class TestItem(scrapy.Item):    id = scrapy.Field()    name = scrapy.Field() 
    description = scrapy.Field()
```

//TODO 待续

####CrawlSpider
####XMLFeedSpider
####CSVFeedSpider
####SitemapSpider


##选择器

###xpath(query)
###css(query)
###re(query)
###extract()
###register_namespace(prefix,uri)
###remove_namespaces()
###\_\_nonzero\_\_()


##Items




